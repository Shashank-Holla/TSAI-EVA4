{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyNJjcqqtvqdWxUtdYXSey+A"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cuz4EuCI5UEL","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","import torch\n","import copy\n","import os\n","from torch.optim.lr_scheduler import _LRScheduler\n","import matplotlib.pyplot as plt\n","\n","class LRFinder(object):\n","    \"\"\"\n","    Input:\n","        model : DNN model\n","        optimizer : optimizer where the defined learning is assumed to be the lower boundary of the range test\n","        criterion : Loss function\n","        device : represents the device on which the computation will take place.\n","        memory_cache : If true, 'state_dict' of the model and optimizer will be cached in memory. Otherwise saved to files under 'cache_dir'\n","    \n","    \"\"\"\n","    \n","    def __init__(self, model, optimizer, criterion, device):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","        self.best_loss = None\n","        \n","        self.history = {\"lr\": [], \"valLoss\": [], \"valAcc\": []}\n","        \n","        if device:\n","            self.device = device\n","        else:\n","            self.device = self.model_device\n","            \n","        # Save the original state of the model and optimizer\n","        self.model_device = next(self.model.parameters()).device\n","        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n","        self.state_cacher.store(\"model\", self.model.state_dict())\n","        self.state_cacher.store(\"optimizer\", self.optimizer.state_dict())\n","        \n","    def reset(self):\n","        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n","\n","        self.model.load_state_dict(self.state_cacher.retrieve(\"model\"))\n","        self.optimizer.load_state_dict(self.state_cacher.retrieve(\"optimizer\"))\n","        self.model.to(self.model_device)\n","        \n","        \n","    \n","    def range_test(self, trainloader, testloader, start_lr=None, end_lr=2, num_iter=100, step_mode=\"linear\", smooth_f=0.05, diverge_th=5, accumulation_steps=1):\n","        \"\"\"\n","        Input:\n","            trainloader : Training set data loader\n","            testloader : Test set data loader\n","            start_lr : starting Learning rate for the range test. (Default=None, uses the learning rate from the optimizer)\n","            end_lr : the last learning rate upto which range test is done. (Default=2)\n","            num_iter : number of iterations over which test occurs. (Default=100)\n","            step_mode : Learning rate policy. Either linear or exponential. (Default=\"linear\")\n","            smooth_f : Loss smoothing factor. [0,1)  (Default=0.05)\n","            diverge_th : test is stopped when loss surpasses the diverge threshold, calculated to be- diverge_th * best_loss (Default=5)\n","            accumulation_steps: steps for gradient accumulation.\n","        Output:\n","            \n","        \"\"\"\n","         \n","        #Reset test results\n","        self.history = {\"lr\": [], \"valLoss\": [], \"valAcc\": []}\n","        self.best_loss = None\n","        \n","        #Move model to device\n","        self.model.to(self.device)\n","        \n","        if start_lr:\n","            self._set_learning_rate(start_lr)\n","        \n","        #Initialize Learning rate policy.Using either \"linear\" or \"exp\". Error otherwise.\n","        if step_mode.lower() == \"linear\":\n","            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n","        elif step_mode.lower() == 'exp':\n","            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n","        else:\n","            raise ValueError(\"Learning rate policy should be either linear or exp. Received {} as the LR policy\".format(step_model))\n","            \n","        \n","        if smooth_f < 0 or smooth_f >= 1:\n","            raise ValueError(\"smooth_f is outside the range [0, 1)\")\n","        \n","        \n","        #Training model begins.\n","        # Iterator to get data by batches.\n","        iter_wrapper = DataLoaderIterWrapper(train_loader)\n","        #Train and test on the batches\n","        for iteration in range(num_iter):\n","            train_loss = self._train_batch(iter_wrapper, accumulation_steps)\n","            test_accuracy, test_loss = self._validate(testloader)\n","        \n","        #Update Learning rate\n","        lr_schedule.step()\n","        self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n","        \n","        # Track the best loss\n","        if iteration == 0:\n","        \tself.best_loss = test_loss\n","        else:\n","        \tif smooth_f > 0:\n","        \t\ttest_loss = smooth_f * test_loss + (1 - smooth_f) * self.history[\"valLoss\"][-1]\n","        \t\t\n","        \tif test_loss < self.best_loss:\n","        \t\tself.best_loss = test_loss\n","\n","        # Check if the loss has diverged; if it has, stop the test\n","        self.history[\"valLoss\"].append(test_loss)\n","        self.history[\"valAcc\"].append(test_accuracy)\n","        \n","        if test_loss > diverge_th * self.best_loss:\n","            print(\"Stopping early, the loss has diverged\")\n","            break\n","\n","        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n","            \n","            \n","            \n","            \n","    \n","    \n","    # Set learning rate.\n","    def _set_learning_rate(self, new_lrs):\n","        if not isinstance(new_lrs, list): # check if its a list\n","            new_lrs = new_lrs * len(self.optimizer.param_groups) #TODO- check this\n","        if len(new_lrs) != len(self.optimizer.param_groups):\n","            raise ValueError(\"Length of new LRs are not equal to number of parameter groups in the optimizer\")\n","        \n","        #TODO- check this\n","        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n","            param_group[\"lr\"] = new_lr\n","            \n","    \n","\n","    #Training the model\n","    def _train_batch(self, iter_wrapper, accumulation_steps):\n","        self.model.train()\n","        total_loss = None\n","        \n","        #Train\n","        self.optimizer.zero_grad()\n","        for i in range(accumulation_steps):\n","            inputs, labels = iter_wrapper.next()\n","            inputs = inputs.to(self.device)\n","            labels = labels.to(self.device)\n","            #Forward pass\n","            outputs = self.model(inputs)\n","            loss = self.criterion(outputs, labels)\n","            #Average loss\n","            loss /= accumulation_steps\n","            #backward pass\n","            loss.backward()\n","            \n","            if total_loss is None:\n","                total_loss = loss\n","            else:\n","                total_loss += loss\n","            \n","        self.optimizer.step()\n","            \n","        return total_loss.item()\n","            \n","    \n","    #Testing the model\n","    def _validate(self, dataloader):\n","        #set in eval mode to disable gradient accumulation\n","        correct = 0\n","\t    total = 0\n","\t    epoch_test_loss = 0.0\n","\t    epoch_test_accuracy = 0\n","        self.model.eval()\n","        with torch.no_grad():\n","            for inputs, labels in dataloader:\n","                inputs = inputs.to(self.device)\n","                labels = labels.to(self.device)\n","                outputs = self.model(inputs)\n","                \n","                if isinstance(inputs, tuple) or isinstance(inputs, list):\n","                    batch_size = inputs[0].size(0)\n","                else:\n","                    batch_size = inputs.size(0)\n","                    \n","                epoch_test_loss += self.criterion(outputs, labels).item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","    \n","    \tepoch_test_accuracy = (100 * correct / total)\n","    \tepoch_test_loss /= len(testloader)     \n","    \treturn epoch_test_accuracy, epoch_test_loss\n","    \n","    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\n","    \t\"\"\"\n","    \tPlot the learning rate range test\n","    \t\n","    \tskip_start : number of batches to trim from the start. (Default=10)\n","    \tskip_end : number of batches to trim from the end. (Default=5)\n","    \tlog_lr : To plot the learning rate graph in logarithmic scale, linear otherwise. (Default=True for log scale)\n","    \tshow_lr : Add a vertical line to visualize the learning rate. (Default=None)\n","    \tax : Matplotlib figure\n","    \t\"\"\"\n","    \t\n","    \tlrs = self.history[\"lr\"]\n","    \tloss = self.history[\"valLoss\"]\n","    \tacc = self.history[\"valAcc\"]\n","    \t\n","    \tif skip_end == 0:\n","    \t\tlrs = lrs[skip_start:]\n","    \t\tloss = loss[skip_start:]\n","    \t\tacc = acc[skip_start:]\n","    \telse:\n","    \t\tlrs = lrs[skip_start:-skip_end]\n","    \t\tloss = loss[skip_start:-skip_end]\n","    \t\tacc = acc[skip_start:-skip_end]\n","    \t\n","    \t#Create figure and axes\n","    \tfig = None\n","    \tif ax is None:\n","    \t\tfig, ax = plt.subplots(1,2,figsize=(15,10))\n","    \t\n","    \t#Plot validation loss and accuracy against Learning rate.\n","    \tax[0].plot(lrs, loss)\n","    \tax[1].plot(lrs, acc)\n","    \tif log_lr:\n","    \t\tax[0].set_xscale(\"log\")\n","    \t\tax[1].set_xscale(\"log\")\n","    \tax[0].set_title(\"Test loss vs Learning rate\")\n","    \tax[0].set_xlabel(\"Learning rate\")\n","        ax[0].set_ylabel(\"Test Loss\")\n","        \n","        ax[1].set_title(\"Test Accuracy vs Learning rate\")\n","        ax[1].set_xlabel(\"Learning rate\")\n","        ax[1].set_ylabel(\"Test Accuracy\")\n","        \n","        if show_lr:\n","        \tax[0].axvline(x=show_lr, color=\"red\")\n","        \tax[1].axvline(x=show_lr, color=\"red\")\n","        \n","        if fig is not None:\n","        \tplt.show()\n","        \n","        return ax\n","        \n","\n","# Setup linear schedule for Learning rate\n","class LinearLR(_LRScheduler):\n","\t\"\"\"\n","\tTo schedule linear learning rate between 2 boundaries over a given number of iterations.\n","\t\n","\tInput:\n","\toptimizer : Optimizer for the model\n","\tend_lr : Final learning rate\n","\tnum_iter : Number of iterations over which test occurs.\n","\tlast_epoch : Index of the final epoch\n","\t\n","\t\"\"\"\n","\tdef __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n","\t\tself.end_lr = end_lr\n","\t\tself.num_iter = num_iter\n","\t\tsuper(LinearLR, self).__init__(optimizer, last_epoch)\n","\t\t\n","\tdef get_lr(self):\n","\t\tcurr_iter = self.last_epoch + 1\n","\t\tr = curr_iter/self.num_iter\n","\t\treturn [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n","\t\t\n","\n","# Setup exponential schedule for learning rate\n","class ExponentialLR(_LRScheduler):\n","    \"\"\"Exponentially increases the learning rate between two boundaries over a number of iterations.\n","    Input:\n","\toptimizer : Optimizer for the model\n","\tend_lr : Final learning rate\n","\tnum_iter : Number of iterations over which test occurs.\n","\tlast_epoch : Index of the final epoch\n","\t\n","    \"\"\"\n","\n","    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n","        self.end_lr = end_lr\n","        self.num_iter = num_iter\n","        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        curr_iter = self.last_epoch + 1\n","        r = curr_iter / self.num_iter\n","        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n"," \n","            \n","# Wrapper to iterate dataloader and provide an option to reset when StopIteration is called.\n","class DataLoaderIterWrapper(object):\n","    \"\"\"\n","    Wrapper to iterate dataloader and provide labels and inputs. Provides functionality to stop in case of divergence (when StopIteration is called.)\n","    \"\"\"\n","    def __init__(self, data_loader, auto_reset=True):\n","        self.data_loader = data_loader\n","        self.auto_reset = auto_reset\n","        self._iterator = iter(data_loader)\n","    \n","    # get new batchsize worth inputs and label.s     \n","    def __next__(self):\n","        try:\n","            inputs, labels = next(self._iterator)\n","        except StopIteration:\n","            if not self.auto_reset:\n","                raise\n","            self._iterator = iter(self.data_loader)\n","            inputs, labels, *_ = next(self._iterator)\n","        return inputs, labels    \n","    \n","        \n","            \n","            \n","            \n","        \n","        "],"execution_count":0,"outputs":[]}]}